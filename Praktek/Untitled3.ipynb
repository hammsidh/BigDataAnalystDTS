{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4ef82b42b2a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mmaxInt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfield_size_limit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxInt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rU'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsv_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '-f'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from pprint import pprint\n",
    "#importing punctuation models in python\n",
    "from string import punctuation as p\n",
    "#import regex\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#arg1 = 'tweet_kotor.csv'\n",
    "#arg2 = 'tweet_bersih.csv'\n",
    "\n",
    "arg1 = sys.argv[1]\n",
    "arg2 = sys.argv[2]\n",
    "\n",
    "\n",
    "def processtweet(tweet):\n",
    "    #process the tweets\n",
    "    \n",
    "    #convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','',tweet)\n",
    "    #delete username*\n",
    "    tweet = re.sub('username[^\\s]+','',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    #punctuation\n",
    "    tweet = re.sub('[0123456789\\!#$%&()*+,-./:;<=>?@^_`{|}~]','',tweet)\n",
    "    #special case for ' \" [ ]\n",
    "    tweet = re.sub(\"'\",\"\",tweet)\n",
    "    tweet = re.sub('\"','',tweet)\n",
    "    tweet = re.sub(\"\\[\",\"\",tweet)\n",
    "    tweet = re.sub(\"\\]\",\"\",tweet)\n",
    "    #delete all unicode\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+','',tweet)\n",
    "    return tweet\n",
    " \n",
    "maxInt = sys.maxsize\n",
    "csv.field_size_limit(maxInt)\n",
    "with open(arg1,'rU') as csv_file:\n",
    "    with open(arg2,'wb') as csv_result:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "\t#next(reader,None) #skipheader\n",
    "        writer = csv.writer(csv_result)\n",
    "\n",
    "        \n",
    "        for line in reader:\n",
    "\t    tweet_array = []\n",
    "\n",
    "            tweets = line[1]\n",
    "            processedTweet = processtweet(tweets)\n",
    "            tweet_array.append(processedTweet)\n",
    "\t    #print processedTweet\n",
    "\t    #print tweets\n",
    "            print tweet_array\n",
    "\t    writer.writerow(tweet_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5fe0ca913b2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tes.xlsx'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m#json_folder = 'pki_2017_09'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mjson1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_one_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;31m#json_folder = extract_from_folder(json_folder)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-5fe0ca913b2b>\u001b[0m in \u001b[0;36mextract_one_file\u001b[0;34m(json_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mjson_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0minstagram_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'www.instagram.com/p'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "#==============================================================================\n",
    "# with open('data.json') as data_file:    \n",
    "#     data = json.load(data_file)\n",
    "#==============================================================================\n",
    "    \n",
    "\n",
    "def extract_one_file(json_path):\n",
    "    json_clean = {}\n",
    "    instagram_path = 'www.instagram.com/p'\n",
    "    with open(json_path) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "    \n",
    "    url = os.path.join(instagram_path,data['shortcode'])\n",
    "    like_count = data['edge_media_preview_like']['count']\n",
    "    comment_count = data['edge_media_to_comment']['count']\n",
    "    comment_text= []\n",
    "    comment_user = []\n",
    "    for item in data['edge_media_to_comment']['edges']:\n",
    "        comment_user.append(item['node']['owner']['username'])\n",
    "        comment_text.append(item['node']['text'])\n",
    "    #comment_user_list = [{'user' : item['node']['owner']['username'],'comment':item['node']['text']} for item in data['edge_media_to_comment']['edges']]\n",
    "    media = data['display_url']\n",
    "    caption = data['edge_media_to_caption']['edges'][0]['node']['text']\n",
    "    hashtag = re.findall(r\"#(\\w+)\", caption)\n",
    "    instagram_id = data['owner']['username']\n",
    "    instagram_alias = data['owner']['full_name']\n",
    "    json_clean['url']=url\n",
    "    json_clean['like_count'] = like_count\n",
    "    json_clean['comment_count'] = comment_count\n",
    "    json_clean['comment_acount'] = comment_user\n",
    "    json_clean['comment_text'] = comment_text\n",
    "    json_clean['media'] = media\n",
    "    json_clean['caption'] = caption\n",
    "    json_clean['hashtag'] = hashtag\n",
    "    json_clean['instagram_id'] = instagram_id\n",
    "    json_clean['instagram_alias'] = instagram_alias\n",
    "    return json_clean\n",
    "\n",
    "\n",
    "def extract_from_folder(json_folder_path):\n",
    "    json_list = glob.glob(json_folder_path+'/*')\n",
    "    json_list = []\n",
    "    for json_file in json_list:\n",
    "        json_list.append(extract_one_file(json_file))\n",
    "        \n",
    "    return json_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    json_file = 'data.json'\n",
    "    filename = 'tes.xlsx'\n",
    "    #json_folder = 'pki_2017_09'\n",
    "    json1 = extract_one_file(json_file)\n",
    "    #json_folder = extract_from_folder(json_folder)\n",
    "    df = pd.DataFrame({k : pd.Series(v) for k, v in json1.iteritems()})\n",
    "    df.to_excel(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
