{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collecting Project\n",
    "## Studi Kasus: Twitter Data Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install library Tweepy\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "# setting key and token\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_token = \"\"\n",
    "access_token_secret = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat object authentication ke API Twitter\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# mensetting access token and secret key\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "# melakukan authentication ke Twitter API\n",
    "api = tweepy.API(auth) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melakukan crawling ke semua data public di home timeline\n",
    "public_tweets = api.home_timeline()\n",
    "\n",
    "for tweets in public_tweets:\n",
    "    print(tweets.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#melihat struktur JSON sebuah data tweet\n",
    "public_tweets[0]._json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melakukan crawling 10 data public di home timeline \n",
    "for tweet in tweepy.Cursor(api.home_timeline).items(10):\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menampilkan nama-nama friends\n",
    "for friend in tweepy.Cursor(api.friends).items():\n",
    "    print(friend.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengcrawl data tweets kita sendiri\n",
    "for tweet in tweepy.Cursor(api.user_timeline).items():\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengcrawl 5 data tweets dari 'nytimes'\n",
    "for tweet in api.user_timeline(id=\"nytimes\", count=5):\n",
    "   print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# menampilkan 5 data hasil crawling detikcom ke dalam format JSON\n",
    "for tweet in api.user_timeline(id=\"detikcom\", count=1):\n",
    "    print(json.dumps(tweet._json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengambil tweets dengan taggar tertentu dg search\n",
    "\n",
    "for tweet in tweepy.Cursor(api.search,q=\"#bigdata\",\n",
    "                           lang=\"en\",\n",
    "                           since=\"2019-01-01\").items(1):\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengambil item tertentu dari JSON tweets ke list\n",
    "tweets_data = []\n",
    "for tweet in tweepy.Cursor(api.search,q=\"#bigdata\",\n",
    "                           lang=\"en\",\n",
    "                           since=\"2019-01-01\").items(5):\n",
    "    tweets_data.append([tweet.author.screen_name,\n",
    "                        tweet.lang,\n",
    "                        tweet.created_at,\n",
    "                        tweet.favorite_count,\n",
    "                        tweet.retweet_count,\n",
    "                        tweet.text,\n",
    "                        tweet.user.location])\n",
    "print(tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list to Dataframe\n",
    "import pandas as pd\n",
    "tweets_pd = pd.DataFrame(tweets_data,\n",
    "                         columns=['screen_name', 'lang', 'created_at', 'fav_count', 'retweet_count', 'text'])\n",
    "display(tweets_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengkonversi dataframe ke file csv\n",
    "bytes_to_write = tweets_pd.to_csv(index = None, header=True).encode()\n",
    "\n",
    "# letakkan file csv ke bucket S3\n",
    "import s3fs\n",
    "s3 = s3fs.S3FileSystem()\n",
    "\n",
    "with s3.open('rosihanari/data_twitter.csv', 'wb') as f:\n",
    "    f.write(bytes_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengambil data streaming dari twitter dari keyword tertentu\n",
    "# data stream dibaca dalam format JSON dan langsung disimpan di Amazon S3\n",
    "\n",
    "import tweepy\n",
    "import json\n",
    "from tweepy.streaming import StreamListener\n",
    "import s3fs\n",
    "s3 = s3fs.S3FileSystem()\n",
    "\n",
    "json_temp = []\n",
    "\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    def __init__(self, api=None):\n",
    "        super(MyStreamListener, self).__init__()\n",
    "        self.num_tweets = 0\n",
    "\n",
    "    def on_status(self, tweet):\n",
    "        \n",
    "        # membatasi jumlah data streaming tweets (n = 5)\n",
    "        self.num_tweets += 1\n",
    "        if self.num_tweets < 5:\n",
    "            # mencetak text status hasil streaming\n",
    "            print(tweet.text)\n",
    "            json_temp.append(tweet._json)\n",
    "            # menyimpan setiap JSON hasil streaming ke S3\n",
    "            with s3.open('rosihanari/data_twitter_json.json', 'ab') as f:\n",
    "                f.write(json.dumps(json_temp).encode('utf-8'))\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "myStreamListener = MyStreamListener()\n",
    "myStream = tweepy.Stream(auth = api.auth, listener=myStreamListener)\n",
    "\n",
    "# streaming keyword='python'\n",
    "myStream.filter(track=['python'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lakukan data collecting terhadap Twitter melalui streaming dengan ketentuan sbb:\n",
    "- Streaming dilakukan pada taggar **#BigData**. \n",
    "- Jumlah data yang dicollect sebanyak 50 buah. \n",
    "- Data yang dicollect adalah '**screen_name**', '**lang**', '**created_at**', '**retweet_count**', '**text**', '**location**'\n",
    "- Tampung data hasil streaming ke dalam list\n",
    "- Konversikan data list hasil streaming ke dalam dataframe\n",
    "- Simpan dataframe hasil streaming ke dalam Amazon S3 dalam format CSV\n",
    "- Lakukan streaming (50 buah data) pada data Twitter dg keyword **#Senin**, data streaming disimpan di file **SeninTwitter.json** (Data stream yang disimpan adalah JSON utuh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
